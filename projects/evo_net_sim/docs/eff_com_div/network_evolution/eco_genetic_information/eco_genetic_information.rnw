\documentclass[12pt]{article}
\usepackage{color}
\usepackage{cite}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
%\usepackage{pdflscape}        %single page landscape
                                %mode \begin{landscape} \end{landscape}
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{/Library/Frameworks/R.framework/Resources/share/texmf/Sweave}
\newcommand{\etal}{\textit{et al.}}
\usepackage{hyperref}  %\hyperref[label_name]{''link text''}
                       %\hyperlink{label}{anchor caption}
                       %\hypertarget{label}{link caption}
\linespread{1.5}

\title{Ecological and Genetic Information Theory}
\author{M.K. Lau}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\thispagestyle{empty}

\setcounter{tocdepth}{3}  %%activate to number sections
\tableofcontents

\section{Main Idea}
\begin{enumerate}
\item Evidence from community genetics studies suggest that ecological
  structure arises in part from genetically determined traits both at
  the scale of individuals, the species that they interact with and
  ecosystem processes.
\item For over 30 years, ecologists have used information theoretic
  tools to estimate the diversity of systems, but have ignored the
  potential these methods provide to examine the flow of information
  in ecosystems.
\item Here, I will explore the potential application of recent
  theoretical developments in systems and information theory to
  examine the dynamics (feedbacks in particular) and limits of the
  transmission of genetic information in ecosystems.
\end{enumerate}

\section{Ulanowicz 2011 Towards quantifying a wider reality: Shannon Exonerata}

\subsection{Information Content of a Signal}

<<>>=
##Functions

##Information Content
SB.H <- function(x,base=exp(1)){
  -sum(sapply(I(table(x)/length(x)),function(x) x*log(x,base=base)))
}

##Internal Order (spatial mutual information)
                                        #I(X;Y) = sum_y in Y sum_x in X P(x,y) log_2 [ p(x,y) / p(x)*p(y) ]
                                        #for each pair of variables calculate their joint and independent probabilities
                                        #p(x,y) = n_xy / 
                                        #p(x) or p(y) = n_x / n
                                        #number of co-occurrences

internal.info <- function(x){
                                        #independent probabilities
  px <- table(x) / length(x)
                                        #joint probabilities
  obs.pairs <- paste(x[x %% 2 == 0],x[x %% 2 == 1],sep='')
  exp.pairs <- paste(as.numeric(expand.grid(0:9,0:9)[,1]),as.numeric(expand.grid(0:9,0:9)[,2]),sep='')
  pairs <- table(c(obs.pairs,exp.pairs))
  pairs[pairs == 1] <- 0
  n.ab <- array(NA,dim=c(length(unique(x)),length(unique(x))))
  rownames(n.ab) <- colnames(n.ab) <- unique(x)
  for (i in 1:length(unique(x))){
    for (j in 1:length(unique(x))){
      n.ab[i,j] <- sum(pairs[names(pairs) == paste(unique(x)[i],unique(x)[j],sep='')|names(pairs) == paste(unique(x)[j],unique(x)[i],sep='')])
    }
  }
                                        #join probability
  p.ab <- n.ab / (length(x) /2)
                                        #mutual information
  A <- p.ab*0
  for (i in 1:length(unique(x))){
    for (j in 1:length(unique(x))){
      A[i,j] <- p.ab[i,j] * log((p.ab[i,j]/(px[as.numeric(names(px)) == unique(x)[i]]*px[as.numeric(names(px)) == unique(x)[j]])),base=2)
    }
  }
                                        #make NA values 0
  A[is.na(A)] <- 0
  return(sum(c(A[upper.tri(A)],diag(A))))
}
                                        #CONDITIONAL ENTROPY
internal.entropy <- function(x){
                                        #independent probabilities
  px <- table(x) / length(x)
                                        #joint probabilities
  obs.pairs <- paste(x[x %% 2 == 0],x[x %% 2 == 1],sep='')
  exp.pairs <- paste(as.numeric(expand.grid(0:9,0:9)[,1]),as.numeric(expand.grid(0:9,0:9)[,2]),sep='')
  pairs <- table(c(obs.pairs,exp.pairs))
  pairs[pairs == 1] <- 0
  n.ab <- array(NA,dim=c(length(unique(x)),length(unique(x))))
  rownames(n.ab) <- colnames(n.ab) <- unique(x)
  for (i in 1:length(unique(x))){
    for (j in 1:length(unique(x))){
      n.ab[i,j] <- sum(pairs[names(pairs) == paste(unique(x)[i],unique(x)[j],sep='')|names(pairs) == paste(unique(x)[j],unique(x)[i],sep='')])
    }
  }
                                        #join probability
  p.ab <- n.ab / (length(x) /2)
                                        #mutual information
  A <- p.ab*0
  for (i in 1:length(unique(x))){
    for (j in 1:length(unique(x))){
      A[i,j] <- p.ab[i,j] * log(((p.ab[i,j]^2)/(px[as.numeric(names(px)) == unique(x)[i]]*px[as.numeric(names(px)) == unique(x)[j]])),base=2)
    }
  }
                                        #make NA values 0
  A[is.na(A)] <- 0
  return((-1*sum(c(A[upper.tri(A)],diag(A)))))
}

@ 

<<>>=
A <- c(4,2,6,0,7,9,5,1,3,6,1,0,3,8,9,8,6,0,7,2,0,6,1,2,4,5,1,3,4,1,2,3,2,3,7,6,7,1,3,6,3,8,4,7,5,1,9,6,0,1,5,5,7,8,2,4,8,4,9,6,8,6,2,0,1,0,0,7,7,4,6,2,2,4,5,2,4,2,0,9,3,7,1,5,9,1,4,4,9,0,4,6,9,4,0,5,6,5,6,0,4,8,0,3,3,8,9,8,6,0,7,2,0,6,1,2,4,5,1,3,4,1,2,3,2,3,7,6,7,1,3,6,3,8,4,7,5,1,9,6,0,1,5,5,7,8,2,4,8,4,9,6,8,6,2,0,1,0,0,7,7,4,6,2,2,4,5,2,4,2,0,9,3,7,1,5,9,1,4,4,9,0,4,6,9,4,0,5,6,5,6,0,4,8,0,3,3,8,9,8)
B <- c(0,3,6,1,7,7,4,6,4,3,9,2,4,2,0,9,3,7,1,5,9,1,4,4,9,0,4,6,9,4,0,5,6,5,6,0,4,8,0,3,3,8,9,8,6,0,7,2,0,6,1,2,4,5,1,3,4,1,2,3,2,3,7,6,7,1,3,6,3,8,4,7,5,1,9,6,0,1,5,5,7,8,2,4,8,4,9,6,8,6,2,0,1,0,0,7,7,4,6,2,2,4,5,2,4,2,0,9,3,7,1,5,9,1,4,4,9,0,4,6,9,4,0,5,6,5,6,0,4,8,0,3,3,8,9,8,6,0,7,2,0,6,1,2,4,5,1,3,4,1,2,3,2,3,7,6,7,1,3,6,3,8,4,7,5,1,9,6,0,1,5,5,7,8,2,4,8,4,9,6,8,6,2,0,1,0,0,7,7,4,6,2,2,4,5,2,4,2,0,9)
C <- c(0,1,4,7,5,6,2,3,8,4,3,7,8,9,6,9,4,7,5,1,7,4,3,1,0,2,3,8,0,3,1,8,1,8,5,4,5,3,8,4,8,9,0,5,2,3,6,4,7,3,2,2,5,9,1,0,9,0,6,4,9,4,1,7,3,7,3,5,5,0,4,1,6,0,2,1,0,1,7,6,8,5,3,2,6,3,0,0,6,7,0,4,6,0,7,2,4,2,4,7,0,9,7,1,8,9,6,9,4,7,5,1,7,4,3,1,0,2,3,8,0,3,1,8,1,8,5,4,5,3,8,4,8,9,0,5,2,3,6,4,7,3,2,2,5,9,1,0,9,0,6,4,9,4,1,7,3,7,7,9,5,0,4,1,1,0,2,1,0,1,7,6,8,5,3,2,6,3,0,0,6,7,0,4,6,0,7,2,4,2,4,7,0,9,7,0,8,9,6,9)


##NOTE: Ulanowicz most likely uses log_2.
H <- c(SB.H(A,2),SB.H(B,2),SB.H(C,2))
round(H,3)


###NOTE: Ulanowicz calculates a percentage. Not sure how, why.

#percent internal info


IF <- c(internal.info(A),internal.info(B),internal.info(C))
IE <- c(internal.entropy(A),internal.entropy(B),internal.entropy(C))
IF + IE

#####NOTE: CHECK THE CALCULATION OF JOINT PROBABILITIES


##Explore unity of A, B and C
piA <- as.numeric(table(A)/sum(table(A)))
piB <- as.numeric(table(B)/sum(table(B)))
piC <- as.numeric(table(C)/sum(table(C)))

pairs(cbind(piA,piB,piC))
cor(cbind(piA,piB,piC))


@ 

\subsection{Mutual Information Content of Two Signals}

<<>>=
  ##


@ 


%\subsection{}

%% %%Figure construction
%% <<echo=false,results=hide,label=fig1,include=false>>=
%% @ 


%% %%Figure plotting
%% \begin{figure} 
%% \begin{center} 
%% <<label=fig1,fig=TRUE,echo=false>>=
%% <<fig1>> 
%% @ 
%% \end{center} 
%% \caption{}
%% \label{fig:one}
%% \end{figure}


%% %%Activate for bibtex vibliography
%% \cite{goossens93}
%% \bibliographystyle{plain}
%% \bibliography{/Users/Aeolus/Documents/bibtex/biblib}


\end{document}  


